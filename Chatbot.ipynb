{"cells":[{"cell_type":"markdown","metadata":{"id":"9fWCV2hnY6RL"},"source":["At first we import the required libraries namely numpy json and nltk. Then we import and read the json file that contains our intent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcKfy16SOwLf"},"outputs":[],"source":["import json\n","import string\n","import random\n","import numpy as np\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense,Dropout\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBELuoBuOwfO"},"outputs":[],"source":["data_root= 'C:/Username/File_location' #replace this with your directory\n","data_file = open(data_root + \"/intents.json\").read()\n","data = json.loads(data_file)\n"]},{"cell_type":"markdown","metadata":{"id":"8xgK28ToY6RX"},"source":["Below, we use nltk in order to organize the intent file into arrays. Then we lemmatize them, which is the proccess that turns all words into their basic root word form, which saves space and makes it easier for our model to learn."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BaTdwze5RRGA"},"outputs":[],"source":["words = []\n","classes = []\n","dataX = []\n","dataY = []\n","for intent in data[\"intents\"]:\n","  for pattern in intent[\"patterns\"]:\n","    tokens = nltk.word_tokenize(pattern)\n","    words.extend(tokens)\n","    dataX.append(pattern)\n","    dataY.append(intent[\"tag\"])\n","  if intent[\"tag\"] not in classes:\n","    classes.append(intent['tag'])\n","lemmatizer = WordNetLemmatizer()\n","words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n","words = sorted(set(words))\n","classes = sorted(set(classes))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJ5DuYc0RRko"},"outputs":[],"source":["training = []\n","out_empty = [0]* len(classes)\n","for idx, doc in enumerate(dataX):\n","  bow = []\n","  text = lemmatizer.lemmatize(doc.lower())\n","  for word in words:\n","    bow.append(1) if word in text else bow.append(0)\n","    output_row = list(out_empty)\n","    output_row[classes.index(dataY[idx])] = 1\n","    training.append([bow,output_row])\n","random.shuffle(training)\n","training = np.array(training, dtype=object)\n","trainX= np.array(list(training[:,0]))\n","trainY=  np.array(list(training[:,1]))"]},{"cell_type":"markdown","metadata":{"id":"xKcUGFquY6Rb"},"source":["Here we create and train a simple neural network model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wrs4O46rTW33"},"outputs":[],"source":["model = Sequential()\n","model.add(Dense(128, input_shape=(len(trainX[0]),),activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(trainY[0]), activation = \"softmax\"))\n","adam = tf.keras.optimizers.Adam(learning_rate=0.01,decay=1e-6)\n","model.compile(loss = 'categorical_crossentropy',\n","              optimizer=adam,\n","              metrics=[\"accuracy\"])\n","print(model.summary())\n","model.fit(x=trainX,y=trainY,epochs=150,verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTaMlWs4ah3Z"},"outputs":[],"source":["def clean_text(text):\n","  tokens = nltk.word_tokenize(text)\n","  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","  return tokens\n","\n","def bag_of_words(text,vocab):\n","  tokens = clean_text(text)\n","  bow = [0]*len(vocab)\n","  for w in tokens:\n","    for idx,word in enumerate(vocab):\n","      if word == w:\n","        bow[idx]= 1\n","  return np.array(bow)\n","\n","def pred_class(text,vocab,labels):\n","  bow = bag_of_words(text,vocab)\n","  result = model.predict(np.array([bow]))[0]\n","  thresh = 0.5\n","  y_pred = [[indx,res]for indx,res in enumerate(result) if res> thresh]\n","  y_pred.sort(key = lambda x: x[1],reverse = True)\n","  return_list = []\n","  for r in y_pred:\n","    return_list.append(labels[r[0]])\n","  return return_list\n","\n","def get_response(intents_list,intents_json):\n","  if len(intents_list) == 0:\n","    result = \"Sorry no bueno\"\n","  else: \n","    tag = intents_list[0]\n","    list_of_intents = intents_json[\"intents\"]\n","    for i in list_of_intents:\n","      if i[\"tag\"] == tag:\n","        result = random.choice(i[\"responses\"])\n","        break\n","  return result"]},{"cell_type":"markdown","metadata":{"id":"8bVEFEe1Y6Re"},"source":["Below is a simple code that allows us to interact with the finished chatbot model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhQD4exMaiL6"},"outputs":[],"source":["print(\"press 0\")\n","while True:\n","  message = input(\"\")\n","  if message == \"0\":\n","    break\n","  intents = pred_class(message,words,classes)\n","  result =  get_response(intents,data)\n","  print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zMYgGa5Cpqw"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}